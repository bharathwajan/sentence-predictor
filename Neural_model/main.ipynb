{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output -- >layer with the neurons=vocabulary\n",
    "take a look at lstm layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_doc(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "in_filename = 'republic_plato_dataset.txt'\n",
    "doc = load_doc(in_filename)\n",
    "tokens = clean_doc(doc)\n",
    "# print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "# print(len(tokens))\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    # print(i-length,i)\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sequences[1].split()),sequences[1],type(sequences[1]),type(sequences) #every sequence has 51 words 50+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences = tokenizer.texts_to_sequences(sequences) #assigns unique number to vocabulary and transforms it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sequences[0]),len(sequences),sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,embedding_dim,hidden_dim,vocab_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) # https://discuss.pytorch.org/t/how-does-nn-embedding-work/88518 --- > each unique word(vocabulary) will have the vector of size(embedding_dim)\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim) # since each word is a time stamp the input for LSTM layer would be equal to embedding_dim        \n",
    "        self.hidden2next = nn.Linear(hidden_dim, vocab_size) # The linear layer that maps from hidden state space to no of unique words in our dataset\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence) # embedding for the words is produced \n",
    "        # print('embeddings',embeds,len(embeds),len(sentence))\n",
    "        # print(\"len of sentence\",sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(1, 1, -1)) #nn.embedding.view --> reshapes the produces embedding to 50 rows and 1 column \n",
    "        final_layer = self.hidden2next(lstm_out.view(1, -1)) # reshapes to 50 rows and one column\n",
    "        next_word_scores = F.log_softmax(final_layer, dim=1) #softmax activation is used because here i need probability distribution for the vocabulary\n",
    "        return next_word_scores\n",
    "embedding_dim=100\n",
    "hidden_dim=100\n",
    "model = LSTM(embedding_dim,hidden_dim,len(set(tokens)))\n",
    "loss_function = nn.NLLLoss() #negative log liklyhood loss -->cross entropy loss #why --> beacuse we had used softmax\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1) #updating the weights after the pass of every sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(tokens)),len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 completed !\n",
      "epoch 1 completed !\n",
      "epoch 2 completed !\n",
      "epoch 3 completed !\n",
      "epoch 4 completed !\n"
     ]
    }
   ],
   "source": [
    "loss_epochs=[]\n",
    "for epoch in range(5):  # running the model for 5 epochs\n",
    "    for seq in sequences[:1000]:\n",
    "        # print(seq)\n",
    "        model.zero_grad() #clearning gradients(change in weight wrt loss) # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        sentence_in = torch.tensor(seq[:50],dtype=torch.long) #taking first 50 words as i/p\n",
    "        next_word = torch.tensor(seq[50],dtype=torch.long) #taking 51st word as output\n",
    "        \n",
    "        ####################3\n",
    "        # tag_scores=model(sentence_in)\n",
    "        # print(tag_scores,len(tag_scores[0]))\n",
    "        ###############\n",
    "        for word in sentence_in:\n",
    "            next_word_score=model(word)\n",
    "        # print(\"output from the model\",tag_scores,len(tag_scores[0]))\n",
    "        loss = loss_function(next_word_score[0],next_word) #computing loss\n",
    "        loss.backward() #backprob for updating weights\n",
    "        optimizer.step() # updating model parameters by calling optimizer.step() function\n",
    "    loss_epochs.append(loss)\n",
    "    print(f\"epoch {epoch} completed !\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of pleasures and desires into necessary and unnecessary these and other great forms of thought are all of them to be found in the republic and were probably first invented by plato the greatest of all logical truths and the one of which writers on philosophy are most apt to lose\n",
      "Prediction is be\n"
     ]
    }
   ],
   "source": [
    "test=sequences[456]\n",
    "print(tokenizer.sequences_to_texts([test])[0])\n",
    "with torch.no_grad():\n",
    "    inputs = torch.tensor(test[:50],dtype=torch.long) \n",
    "    for word in inputs:\n",
    "        tag_scores=model(word)\n",
    "    # print(len(tag_scores[0]),type(tag_scores[0])) #prob distribution for the entire vocab\n",
    "    idx=torch.argmax(tag_scores[0])\n",
    "    # print()\n",
    "    temp={val:key for key,val in tokenizer.word_index.items()}\n",
    "    print('Prediction is',temp[int(idx)])\n",
    "    # temp={val:key for key,val in tokenizer.word_index.items()}\n",
    "    # print('next word is',temp[idx])\n",
    "    # # print(max,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imaginary'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9c62d8cf6e30c1639f8e9a0a261a530483935ac00f94e2882c1ade2d532fd4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
